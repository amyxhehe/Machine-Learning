{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class My_SVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, iteration=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param #to prevent overthinking- overfitting\n",
        "        self.i = iteration\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "#weights & biases -> drawing line\n",
        "#hinge loss & gradient descent -> fixing mistakes\n",
        "    def fit(self, X, Y):\n",
        "        n_rows, n_columns = X.shape\n",
        "\n",
        "        #if y[i] <= 0, replace it with -1 otherwise, replace it with 1\n",
        "        y = np.where(Y <= 0, -1, 1)  #ensure labels are -1 or 1\n",
        "\n",
        "        self.w = np.zeros(n_columns) #1 weight per feature\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.i):\n",
        "            #x is a single training example\n",
        "            #y[index] is its corresponding label (-1 or 1)\n",
        "            for index, x in enumerate(X):\n",
        "                #checks if sample is on correct side of the margin (distance â‰¥ 1))\n",
        "                #np.dot(x, w) + b is the current prediction score\n",
        "                #distance = y[index] * (score) is positive if correct class, negative if not\n",
        "                condition = y[index] * (np.dot(x, self.w) + self.b) >= 1 #hinge condition\n",
        "\n",
        "                if condition: #is true i.e. well classified\n",
        "                #regularization i.e. pulling weights slightly toward zero to prevent overfitting\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                #regularization + hinge loss correction to fix the misclassification\n",
        "                #to move the decision boundary to better separate this sample\n",
        "                #subtracting a portion of the gradient of the hinge loss\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x, y[index]))\n",
        "                    self.b -= self.lr * y[index] #moves the boundary up or down depending on the class label\n",
        "\n",
        "    def predict(self, X):\n",
        "        result = np.dot(X, self.w) + self.b\n",
        "        return np.sign(result)"
      ],
      "metadata": {
        "id": "5GreA3LLWdM6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [2, 3],\n",
        "    [1, 1],\n",
        "    [2, 1],\n",
        "    [5, 4],\n",
        "    [6, 5],\n",
        "    [7, 8]\n",
        "])\n",
        "Y = np.array([-1, -1, -1, 1, 1, 1])\n",
        "\n",
        "model = My_SVM()\n",
        "model.fit(X, Y)\n",
        "results = model.predict(X)\n",
        "\n",
        "print(\"Weights:\", model.w)\n",
        "print(\"Bias:\", model.b)\n",
        "print(\"Predictions:\", results)\n",
        "#super bad results buts its okay- we just needed to study the mathematical basis of the model :p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZmxtLs5WelO",
        "outputId": "0a282e03-3f3c-4455-e7a6-eed3f50f56eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [-0.17729676  0.01223383]\n",
            "Bias: 2.2289999999998655\n",
            "Predictions: [1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    }
  ]
}